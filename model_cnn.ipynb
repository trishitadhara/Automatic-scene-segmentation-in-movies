{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p7RKVA3zg-u",
        "outputId": "d6b5ab2d-d257-4b89-9114-a747df30ee68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import pdb\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8EwxijKXcDq",
        "outputId": "a6acca89-9ab2-42e0-a3c3-2363ae91efd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  2.0.0+cu118\n",
            "Torchvision Version:  0.15.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_random=pd.read_csv('/content/scene318/label318/tt0047396.txt',sep =\" \")"
      ],
      "metadata": {
        "id": "slpSPPJzwJzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf /content/gdrive/MyDrive/240P/train/tt0047396.tar"
      ],
      "metadata": {
        "id": "pM9XyRTrjkCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(df_random[\"0\"])"
      ],
      "metadata": {
        "id": "DCXn6F2XwoVu",
        "outputId": "ab2cbcb8-973a-4bb8-b92c-9b49143c88b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 669, 1: 116, -1: 6})"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir='/content/tt0047396'"
      ],
      "metadata": {
        "id": "IDHSTzs3nPw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = sorted(os.listdir(data_dir))"
      ],
      "metadata": {
        "id": "mw4K5PTJad9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/scene318-20230427T183512Z-001.zip"
      ],
      "metadata": {
        "id": "YN4Mu4VUeOz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os.path as osp"
      ],
      "metadata": {
        "id": "SqzJIvAHelmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scene_json=pd.read_json('/content/scene318/meta/scene_movie318.json')"
      ],
      "metadata": {
        "id": "klA9qbK1ecRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root='/content/scene318/'"
      ],
      "metadata": {
        "id": "ARFakXtXfxhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "class MovieNetDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.df = self.df.loc[self.df.index.repeat(3)].reset_index(drop=True)\n",
        "        self.root_dir = root_dir\n",
        "        # print(len(sorted(os.listdir(self.root_dir))))\n",
        "        # print(len(self.df))\n",
        "        self.transform = transform\n",
        "    '''def transform(self):\n",
        "      data_transforms = {\n",
        "     transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    }'''\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        image_st=[]\n",
        "        for ids in range(0,6,1):\n",
        "          if ids+idx<0 or ids+idx>=2370:\n",
        "            continue\n",
        "          img_name = sorted(os.listdir(self.root_dir))[idx+ids]\n",
        "          \n",
        "          image = io.imread(self.root_dir+\"/\"+img_name)\n",
        "          #print(type(image))\n",
        "          image = self.transform(image)\n",
        "          image_st.append(image)\n",
        "        #print(image)\n",
        "        image=torch.stack(image_st)\n",
        "        labels = np.array([self.df['label'][idx+3]])\n",
        "\n",
        "        sample = {'image': torch.tensor(image), 'label': torch.tensor(labels)}\n",
        "\n",
        "        \n",
        "        return sample"
      ],
      "metadata": {
        "id": "VKkuHq59nTCe"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_dataset = MovieNetDataset(csv_file='/content/gdrive/MyDrive/tt0047396.csv',\n",
        "                                    root_dir='/content/tt0047396',transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        \n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]))\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "# for i in range(len(face_dataset)):\n",
        "#     sample = face_dataset[i]\n",
        "#     print(sample['image'].shape)\n",
        "#     print(i, sample['image'].shape, sample['label'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "caKDxccZnqUp",
        "outputId": "fa981b38-0003-4de2-8d50-72f0576b415e"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Cos(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Cos, self).__init__()\n",
        "        self.shot_num = 6\n",
        "        self.channel = 512\n",
        "        self.conv1 = nn.Conv2d(1, self.channel, kernel_size=(self.shot_num//2, 1))\n",
        "\n",
        "    def forward(self, x):  # [batch_size, seq_len, shot_num, feat_dim]\n",
        "        #print(x.shape)\n",
        "        x = x.view(-1, 1, x.shape[2], x.shape[3])\n",
        "        #print(\"cos_reshape\")\n",
        "        #print(x.shape)\n",
        "        part1, part2 = torch.split(x, [self.shot_num//2]*2, dim=2)\n",
        "        #print(part1.shape)\n",
        "        # batch_size*seq_len, 1, [self.shot_num//2], feat_dim\n",
        "        part1 = self.conv1(part1).squeeze()\n",
        "        part2 = self.conv1(part2).squeeze()\n",
        "        #print(part1.shape)\n",
        "        x = F.cosine_similarity(part1, part2, dim=1)  # batch_size,channel\n",
        "        # x = F.cosine_similarity(part1, part2, dim=1)  # batch_size,channel\n",
        "        # x = F.cosine_similarity(part1, part2, dim=2)  # batch_size,channel\n",
        "        # x = F.cosine_similarity(part1, part2, dim=3)  # batch_size,channel\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "iajbxV28Vz9k"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNet, self).__init__()\n",
        "        self.shot_num = 6\n",
        "        self.channel = 512\n",
        "        self.conv1 = nn.Conv2d(1, self.channel, kernel_size=(6, 1))\n",
        "        self.max3d = nn.MaxPool3d(kernel_size=(self.channel, 1, 1))\n",
        "        self.cos = Cos()\n",
        "        model_ft = models.resnet18(pretrained=True)\n",
        "        model_ft.fc = nn.Identity()\n",
        "        model_ft.eval()\n",
        "        self.feat_extractor = model_ft\n",
        "\n",
        "    def forward(self, x):  # [batch_size, seq_len, shot_num, 3, 224, 224]\n",
        "        #print(x.shape)\n",
        "        #print(type(x))\n",
        "        feat = self.feat_extractor(x.float())\n",
        "        #print(feat.shape)\n",
        "        #feat = feat.view(feat.size(0), -1)\n",
        "        feat =  feat.view(1, 1, 6, -1)\n",
        "        \n",
        "        #print(\"feat\",feat)\n",
        "        # [batch_size, seq_len, shot_num, feat_dim] \n",
        "        context=feat\n",
        "        context = feat.view(feat.shape[0]*feat.shape[1], 1, feat.shape[-2], feat.shape[-1])\n",
        "        #print(\"context\",context)\n",
        "        context = self.conv1(context)\n",
        "        # batch_size*seq_len,sim_channel,1,feat_dim\n",
        "        context = self.max3d(context)\n",
        "        # batch_size*seq_len,1,1,feat_dim\n",
        "        context = context.squeeze()\n",
        "        sim = self.cos(feat)\n",
        "        #print(context.shape,sim.shape)\n",
        "        bound = torch.cat((context, sim))\n",
        "        return bound\n"
      ],
      "metadata": {
        "id": "QEKlbrAmTUBH"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LGSSone(nn.Module):\n",
        "    def __init__(self, mode=\"image\"):\n",
        "        super(LGSSone, self).__init__()\n",
        "        self.seq_len = 1\n",
        "        self.num_layers = 1\n",
        "        self.lstm_hidden_size = 512\n",
        "        if mode == \"image\":\n",
        "            self.bnet = BNet()\n",
        "            self.input_dim = (512+\n",
        "                512)\n",
        "\n",
        "        self.fc1 = nn.Linear(5632, 100)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x)\n",
        "        x = self.bnet(x)\n",
        "        x = x.view(-1, self.seq_len, x.shape[-1])\n",
        "        # torch.Size([128, seq_len, 3*channel])\n",
        "        #self.lstm.flatten_parameters()\n",
        "        #out, (_, _) = self.lstm(x, None)\n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        #print(out.shape)\n",
        "        #out=out.flatten()\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        #out = out.view(-1, 2)\n",
        "        return torch.sigmoid(out)"
      ],
      "metadata": {
        "id": "NeZmiIhqWq2n"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LGSS_image(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LGSS_image, self).__init__()\n",
        "        #self.mode = cfg.dataset.mode\n",
        "        #if 'image' in self.mode:\n",
        "        self.bnet_image = LGSSone(\"image\")\n",
        "\n",
        "    def forward(self, img):\n",
        "        out = 0\n",
        "        image_bound = self.bnet_image(img)\n",
        "        out = image_bound\n",
        "        return out"
      ],
      "metadata": {
        "id": "N-_7mtM73HL-"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LGSS_image()"
      ],
      "metadata": {
        "id": "wJsEPUAu2mGO"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "xjpsF5HS3c5e"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(2):\n",
        "  c=0\n",
        "  for i in range(0,2000,1):\n",
        "    '''\n",
        "    imgs=[]\n",
        "    labels=[]\n",
        "    for j in range(i,i+10):\n",
        "      img=(face_dataset[j]['image'])\n",
        "      imgs.append(img)\n",
        "      label=(face_dataset[j]['label'])\n",
        "      labels.append(label)\n",
        "    imgs=torch.stack(imgs)'''\n",
        "    img=(face_dataset[i]['image'])\n",
        "    labels=(face_dataset[i]['label'])\n",
        "\n",
        "    imgs=torch.reshape(imgs,(60, 3, 224, 224))\n",
        "    labels=torch.tensor(labels)\n",
        "    #print(img.dtype)\n",
        "    #print(img,labels)\n",
        "    optimizer.zero_grad()\n",
        "    output=model(imgs)\n",
        "    #print(output.flatten())\n",
        "    #print(labels)\n",
        "    output=output.flatten()\n",
        "    for k in range(1):\n",
        "      if torch.round(output[k])==labels[k]:\n",
        "        c+=1\n",
        "    loss=criterion(output,labels.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #print(c)\n",
        "    #print(loss.item())\n",
        "    if i%50==0 and i>0:\n",
        "      print(f\"Epoch: {epoch},batch_processed: {i}/2000, train_acc: {c/i}\")\n"
      ],
      "metadata": {
        "id": "DlMcXa_tVXix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'/content/gdrive/MyDrive/model_cnn.h5')"
      ],
      "metadata": {
        "id": "ckY-Z7K9BZNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=torch.load('/content/gdrive/MyDrive/model_cnn.h5')"
      ],
      "metadata": {
        "id": "mx0nA0jpBbMx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "XH4l1t_jBa23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uC_pwoQMB7A4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score"
      ],
      "metadata": {
        "id": "k1HH-YTeCWaF"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c=0\n",
        "preds=[]\n",
        "l=[]\n",
        "for i in range(2001,2350,10):\n",
        "    imgs=[]\n",
        "    labels=[]\n",
        "    #c=0\n",
        "    for j in range(i,i+10):\n",
        "      img=(face_dataset[j]['image'])\n",
        "      imgs.append(img)\n",
        "      label=(face_dataset[j]['label'])\n",
        "      labels.append(label)\n",
        "    imgs=torch.stack(imgs)\n",
        "    imgs=torch.reshape(imgs,(60, 3, 224, 224))\n",
        "    labels=torch.tensor(labels)\n",
        "    #print(img.dtype)\n",
        "    #print(img,labels)\n",
        "    optimizer.zero_grad()\n",
        "    output=model(imgs)\n",
        "    #print(output.flatten())\n",
        "    #print(labels)\n",
        "    output=output.flatten()\n",
        "    threshold=0.006\n",
        "    #print(output)\n",
        "    output = torch.where(output < threshold, torch.zeros_like(output), torch.ones_like(output))\n",
        "    #print(output)\n",
        "    #print(labels)\n",
        "    for k in range(10):\n",
        "      #print(output[k],labels[k])\n",
        "      if output[k]==labels[k]: \n",
        "        c+=1\n",
        "    gt = torch.round(output)\n",
        "    #print(gt.detach().numpy(),labels)\n",
        "    #break\n",
        "    preds.append(gt.detach().numpy())\n",
        "    l.append(labels.numpy())\n",
        "    precision = average_precision_score(gt.detach().numpy(),labels) \n",
        "    #loss=criterion(output,labels.float())\n",
        "    #loss.backward()\n",
        "    #optimizer.step()\n",
        "    #print(c,i)\n",
        "    #print(loss.item())\n",
        "    #print(f\"Epoch: {epoch},batch_processed: {i-2000/350}, test_acc: {c/(i-2000)},precision: {precision}\")\n",
        "    if i%10==1 and i>2001:\n",
        "      print(f\"batch_processed: {(i-2000)/350}, test_acc: {c/(i-1991)},precision: {precision}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WstjDsKZxhnP",
        "outputId": "f178e2b6-a613-44f3-82b4-1809ac412d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_processed: 0.03142857142857143, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.06, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.08857142857142856, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.11714285714285715, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.1457142857142857, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.1742857142857143, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.20285714285714285, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.23142857142857143, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.26, test_acc: 0.8,precision: 0.2\n",
            "batch_processed: 0.2885714285714286, test_acc: 0.7909090909090909,precision: 0.3\n",
            "batch_processed: 0.3171428571428571, test_acc: 0.7916666666666666,precision: 0.5444444444444444\n",
            "batch_processed: 0.3457142857142857, test_acc: 0.7846153846153846,precision: 0.43333333333333335\n",
            "batch_processed: 0.3742857142857143, test_acc: 0.7357142857142858,precision: 0.24166666666666667\n",
            "batch_processed: 0.40285714285714286, test_acc: 0.72,precision: 0.5\n",
            "batch_processed: 0.43142857142857144, test_acc: 0.71875,precision: 0.3\n",
            "batch_processed: 0.46, test_acc: 0.711764705882353,precision: 0.4\n",
            "batch_processed: 0.48857142857142855, test_acc: 0.6722222222222223,precision: 1.0\n",
            "batch_processed: 0.5171428571428571, test_acc: 0.6368421052631579,precision: 1.0\n",
            "batch_processed: 0.5457142857142857, test_acc: 0.63,precision: 0.5\n",
            "batch_processed: 0.5742857142857143, test_acc: 0.6333333333333333,precision: 0.3\n",
            "batch_processed: 0.6028571428571429, test_acc: 0.6227272727272727,precision: 0.6\n",
            "batch_processed: 0.6314285714285715, test_acc: 0.6086956521739131,precision: 0.7\n",
            "batch_processed: 0.66, test_acc: 0.6041666666666666,precision: 0.5\n",
            "batch_processed: 0.6885714285714286, test_acc: 0.612,precision: 0.2\n",
            "batch_processed: 0.7171428571428572, test_acc: 0.6192307692307693,precision: 0.2\n",
            "batch_processed: 0.7457142857142857, test_acc: 0.6037037037037037,precision: 0.2\n",
            "batch_processed: 0.7742857142857142, test_acc: 0.6071428571428571,precision: 0.3\n",
            "batch_processed: 0.8028571428571428, test_acc: 0.603448275862069,precision: 0.5\n",
            "batch_processed: 0.8314285714285714, test_acc: 0.61,precision: 0.2\n",
            "batch_processed: 0.86, test_acc: 0.6161290322580645,precision: 0.2\n",
            "batch_processed: 0.8885714285714286, test_acc: 0.621875,precision: 0.2\n",
            "batch_processed: 0.9171428571428571, test_acc: 0.6272727272727273,precision: 0.2\n",
            "batch_processed: 0.9457142857142857, test_acc: 0.6323529411764706,precision: 0.2\n",
            "batch_processed: 0.9742857142857143, test_acc: 0.6371428571428571,precision: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AP = average_precision_score(preds,l)"
      ],
      "metadata": {
        "id": "mf3WgMOQxuxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vT0yp4L96nc",
        "outputId": "e4279457-7a63-45d9-d0f1-6898a82b0859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3274055829228243"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    }
  ]
}